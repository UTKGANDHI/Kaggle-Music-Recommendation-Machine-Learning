{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MLProjectNN\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexData(df_sample):   \n",
    "    df_sche = df_sample.schema.fields\n",
    "    for s in df_sche:\n",
    "        n = s.name\n",
    "        if (n !=\"target\")&(n!=\"id\"):\n",
    "            print(n)\n",
    "            indexer = StringIndexer(inputCol=n, outputCol=n+\"_index\").fit(df_sample)\n",
    "            df_sample = indexer.transform(df_sample).drop(n)\n",
    "        elif n==\"id\":\n",
    "            indexer = StringIndexer(inputCol=n, outputCol=n+\"_index\").fit(df_sample)\n",
    "            df_sample = indexer.transform(df_sample)\n",
    "    return df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assembleData(df_sample,case):\n",
    "    vecAssembler = VectorAssembler(inputCols = [\"msno_index\",\"song_id_index\",\"source_system_tab_index\",\n",
    "            \"source_screen_name_index\",\"source_type_index\"], outputCol = \"features\")\n",
    "    df_0 = vecAssembler.transform(df_sample)\n",
    "    #turn sparse vectors to dense vectors\n",
    "    if case==\"train\":\n",
    "        df_1 = df_0.withColumn(\"target\",df_0[\"target\"].cast(\"double\")).select(\"target\",\"features\")\n",
    "        return df_1\n",
    "    elif case==\"test\":\n",
    "        df_1 = df_0.select(\"features\",\"id\")\n",
    "        return df_1\n",
    "    else:\n",
    "        print(\"No such type\")\n",
    "        return df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno\n",
      "song_id\n",
      "source_system_tab\n",
      "source_screen_name\n",
      "source_type\n",
      "msno\n",
      "song_id\n",
      "source_system_tab\n",
      "source_screen_name\n",
      "source_type\n"
     ]
    }
   ],
   "source": [
    "df_train = spark.read.csv(\"/Users/Bobby/Documents/class/Machine Learning/finalProject/train.csv\",\n",
    "                header=\"true\")\n",
    "df_test = spark.read.csv(\"/Users/Bobby/Documents/class/Machine Learning/finalProject/test.csv\",\n",
    "                header=\"true\")\n",
    "# df.printSchema()\n",
    "# print(df_sample.count())\n",
    "df_train_sample = df_train.sample(False, 0.001, None).na.fill('unknown')\n",
    "df_test_sample = df_test.na.fill('unknown')\n",
    "indexed_train_data = indexData(df_train_sample)\n",
    "indexed_test_data = indexData(df_test_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7145\n",
      "7145\n",
      "+------+----------+-------------+-----------------------+------------------------+-----------------+\n",
      "|target|msno_index|song_id_index|source_system_tab_index|source_screen_name_index|source_type_index|\n",
      "+------+----------+-------------+-----------------------+------------------------+-----------------+\n",
      "|     1|     195.0|        654.0|                    0.0|                     0.0|              2.0|\n",
      "|     0|    3049.0|       2753.0|                    1.0|                     1.0|              1.0|\n",
      "|     1|    1322.0|       3494.0|                    0.0|                     0.0|              0.0|\n",
      "|     1|    4280.0|       3777.0|                    0.0|                     0.0|              2.0|\n",
      "|     1|     268.0|         89.0|                    0.0|                     0.0|              0.0|\n",
      "|     1|     526.0|       2644.0|                    0.0|                     0.0|              0.0|\n",
      "|     1|    4511.0|       3538.0|                    0.0|                     0.0|              0.0|\n",
      "|     1|    5477.0|       1449.0|                    0.0|                     0.0|              2.0|\n",
      "|     0|    2181.0|       3165.0|                    0.0|                     0.0|              0.0|\n",
      "|     1|    3903.0|       2866.0|                    0.0|                     0.0|              0.0|\n",
      "+------+----------+-------------+-----------------------+------------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_obj = df1.join(df2,df1.msno == df2.msno,how=\"right\").show(10)\n",
    "print(indexed_train_data.count())\n",
    "print(indexed_train_data.dropna().count())\n",
    "indexed_train_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = assembleData(indexed_train_data,\"train\")\n",
    "df_test = assembleData(indexed_test_data,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|            features| id|\n",
      "+--------------------+---+\n",
      "|[19036.0,2049.0,0...|  0|\n",
      "|[19036.0,114.0,0....|  1|\n",
      "|[24643.0,108157.0...|  2|\n",
      "|[2123.0,23014.0,3...|  3|\n",
      "|[2123.0,75657.0,3...|  4|\n",
      "|[2123.0,12444.0,3...|  5|\n",
      "|[2123.0,7053.0,3....|  6|\n",
      "|[2123.0,1823.0,0....|  7|\n",
      "|[2123.0,1608.0,3....|  8|\n",
      "|[9739.0,460.0,0.0...|  9|\n",
      "+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_train.show(10)\n",
    "df_test.show(10)\n",
    "# print(df_test.count())\n",
    "# print(df_test.dropna().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 2]\n",
      "0.5370670157847907\n",
      "[5, 4, 2]\n",
      "0.5399133345615184\n",
      "[5, 5, 2]\n",
      "0.5220026732622814\n",
      "best layer:\n",
      "[5, 4, 2]\n",
      "max accuracy:\n",
      "0.5399133345615184\n"
     ]
    }
   ],
   "source": [
    "#set parameters for a KNN model\n",
    "\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "layers = [[5,3,2],[5,4,2],[5,5,2]]\n",
    "maxAccuracy = 0\n",
    "bestLayer = []\n",
    "\n",
    "for layer in layers:\n",
    "    trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layer, blockSize=128)\n",
    "    param = trainer.setParams(featuresCol = \"features\",labelCol=\"target\")\n",
    "    #use K-Fold validation to tune the model\n",
    "    #pyspark library\n",
    "    grid = ParamGridBuilder().build()\n",
    "    # .addGrid(trainer.maxIter, [0, 1]) random forest\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"target\")\n",
    "    cv = CrossValidator(estimator=trainer, estimatorParamMaps=grid, evaluator=evaluator,numFolds=5)\n",
    "    cv.extractParamMap()\n",
    "    cvModel = cv.fit(df_train)\n",
    "    print(layer)\n",
    "    cvModel.avgMetrics[0]\n",
    "    print(evaluator.evaluate(cvModel.transform(df_train)))\n",
    "    if(evaluator.evaluate(cvModel.transform(df_train))>maxAccuracy):\n",
    "        bestLayer = layer\n",
    "        maxAccuracy = evaluator.evaluate(cvModel.transform(df_train))\n",
    "print(\"best layer:\")\n",
    "print(bestLayer)\n",
    "print(\"max accuracy:\")\n",
    "print(maxAccuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=bestLayer, blockSize=128,labelCol=\"target\")\n",
    "model = trainer.fit(df_train)\n",
    "result = model.transform(df_test)\n",
    "df_final = result.selectExpr(\"id as id\",\"prediction as target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.coalesce(1).write.csv(\"/Users/Bobby/Documents/class/Machine Learning/finalProject/testResult.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
